{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_in_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZDCxj8RT6k_7",
        "dvOe_5yUC6rV",
        "dW_pHyeVZQJm",
        "fdfUzRDPaEA8",
        "gwqN2EPybIoX",
        "0tRDtWDwdDAN",
        "Nn48sRpngAcf",
        "TvucM-3DhJA1",
        "EZtZ17QHi1Da",
        "IqQhbwywnbTs",
        "1nl1ylgZrz-E"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY",
        "colab_type": "text"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "This installs Apache Spark 2.4, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c822bd5-4194-409d-9e97-c661d1bab6f7"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-2.4.6-bin-hadoop2.7/\n",
            "spark-2.4.6-bin-hadoop2.7/bin/\n",
            "spark-2.4.6-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-2.4.6-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/beeline\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.4.6-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.4.6-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/run-example\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.4.6-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.4.6-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.4.6-bin-hadoop2.7/README.md\n",
            "spark-2.4.6-bin-hadoop2.7/R/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.4.6-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.4.6-bin-hadoop2.7/NOTICE\n",
            "spark-2.4.6-bin-hadoop2.7/data/\n",
            "spark-2.4.6-bin-hadoop2.7/data/graphx/\n",
            "spark-2.4.6-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.4.6-bin-hadoop2.7/data/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-jtransforms.html\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-vis-timeline.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.4.6-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-2.4.6-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-2.4.6-bin-hadoop2.7/examples/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderEstimatorExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_estimator_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderEstimatorExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.4.6-bin-hadoop2.7/examples/jars/\n",
            "spark-2.4.6-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/examples/jars/scopt_2.11-3.7.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/LICENSE\n",
            "spark-2.4.6-bin-hadoop2.7/RELEASE\n",
            "spark-2.4.6-bin-hadoop2.7/python/\n",
            "spark-2.4.6-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.4.6-bin-hadoop2.7/python/README.md\n",
            "spark-2.4.6-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.4.6-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_coverage/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.4.6-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.4.6-bin-hadoop2.7/python/setup.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/lib/\n",
            "spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip\n",
            "spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.4.6-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.4.6-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/dist/\n",
            "spark-2.4.6-bin-hadoop2.7/python/.coveragerc\n",
            "spark-2.4.6-bin-hadoop2.7/python/run-tests\n",
            "spark-2.4.6-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.4.6-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/test_serializers.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/test_broadcast.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.4.6-bin-hadoop2.7/yarn/\n",
            "spark-2.4.6-bin-hadoop2.7/yarn/spark-2.4.6-yarn-shuffle.jar\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.4.6-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.4.6-bin-hadoop2.7/jars/\n",
            "spark-2.4.6-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/kubernetes-client-4.6.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-core-2.6.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-tags_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/janino-3.0.16.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/kubernetes-model-common-4.6.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/scala-library-2.11.12.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/chill-java-0.9.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-core_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-repl_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/orc-core-1.5.5-nohive.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-databind-2.6.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/kubernetes-model-4.6.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-hive_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/metrics-core-3.1.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/netty-all-4.1.47.Final.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/xz-1.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/metrics-json-3.1.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-compiler-3.0.16.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/httpcore-4.4.10.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/arrow-format-0.10.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-compress-1.8.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/orc-mapreduce-1.5.5-nohive.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/orc-shims-1.5.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hppc-0.7.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-sql_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/py4j-0.10.7.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/snakeyaml-1.15.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/snappy-java-1.1.7.5.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/okio-1.15.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-tags_2.11-2.4.6-tests.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/okhttp-3.12.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.6.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/lz4-java-1.4.0.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.4.6-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar\n",
            "spark-2.4.6-bin-hadoop2.7/conf/\n",
            "spark-2.4.6-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.4.6-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.4.6-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.4.6-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.4.6-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.4.6-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.4.6-bin-hadoop2.7/conf/spark-defaults.conf.template\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl",
        "colab_type": "text"
      },
      "source": [
        "# Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnb_ePUyQIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "from collections import namedtuple"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P",
        "colab_type": "text"
      },
      "source": [
        "# Start a SparkSession\n",
        "This will start a local Spark session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "from pyspark.sql import Row\n",
        "from pyspark import *"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJp8ZI-VzYEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b3d4c3ea-a0a8-4c2c-c670-13e93f297d74"
      },
      "source": [
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SCOxxIxbfwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_top10(rdd):\n",
        "  for i in rdd.take(10):\n",
        "    print(i)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkpzPTYX6z3C",
        "colab_type": "text"
      },
      "source": [
        "# Data Download\n",
        "\n",
        "The data of https://github.com/dgadiraju/retail_db.git will be used for demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZkw_gPEQvId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a3fa709-616f-4b0e-e243-75f219ab4770"
      },
      "source": [
        "!git clone https://github.com/dgadiraju/retail_db.git"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'retail_db' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1fBOVv5kXX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fb133a66-0878-4e6a-9215-29501f9f06c0"
      },
      "source": [
        "!ls ./retail_db/products -l"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 172\n",
            "-rw-r--r-- 1 root root 174155 Jul  3 08:31 part-00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZzTxe78ksH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc=spark.sparkContext"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEn8AuXj6vjr",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation\n",
        "<Font color=red>**Please don't spend too much time on RDD, which is not required for the exam CCA Spark and Hadoop Developer Exam (CCA175).**</Font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDCxj8RT6k_7",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation - Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA31ur8g8eIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "products_txt=sc.textFile(\"././retail_db/products/part-00000\")"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNKOk9_DkKPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "16051cbb-dc47-4f64-a66a-9afc3509079f"
      },
      "source": [
        "products_txt.count()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1345"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjYiCiI3lTp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bb3902f5-2cc0-4d58-8ce1-65599434b4a0"
      },
      "source": [
        "for i in products_txt.take(10):\n",
        "  print(i)\n",
        "  for j in enumerate(i.split(\",\")):\n",
        "   print(j[0],\":\", j[1])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy\n",
            "0 : 1\n",
            "1 : 2\n",
            "2 : Quest Q64 10 FT. x 10 FT. Slant Leg Instant U\n",
            "3 : \n",
            "4 : 59.98\n",
            "5 : http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy\n",
            "2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n",
            "0 : 2\n",
            "1 : 2\n",
            "2 : Under Armour Men's Highlight MC Football Clea\n",
            "3 : \n",
            "4 : 129.99\n",
            "5 : http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n",
            "3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
            "0 : 3\n",
            "1 : 2\n",
            "2 : Under Armour Men's Renegade D Mid Football Cl\n",
            "3 : \n",
            "4 : 89.99\n",
            "5 : http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
            "4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
            "0 : 4\n",
            "1 : 2\n",
            "2 : Under Armour Men's Renegade D Mid Football Cl\n",
            "3 : \n",
            "4 : 89.99\n",
            "5 : http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
            "5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet\n",
            "0 : 5\n",
            "1 : 2\n",
            "2 : Riddell Youth Revolution Speed Custom Footbal\n",
            "3 : \n",
            "4 : 199.99\n",
            "5 : http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet\n",
            "6,2,Jordan Men's VI Retro TD Football Cleat,,134.99,http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat\n",
            "0 : 6\n",
            "1 : 2\n",
            "2 : Jordan Men's VI Retro TD Football Cleat\n",
            "3 : \n",
            "4 : 134.99\n",
            "5 : http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat\n",
            "7,2,Schutt Youth Recruit Hybrid Custom Football H,,99.99,http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014\n",
            "0 : 7\n",
            "1 : 2\n",
            "2 : Schutt Youth Recruit Hybrid Custom Football H\n",
            "3 : \n",
            "4 : 99.99\n",
            "5 : http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014\n",
            "8,2,Nike Men's Vapor Carbon Elite TD Football Cle,,129.99,http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat\n",
            "0 : 8\n",
            "1 : 2\n",
            "2 : Nike Men's Vapor Carbon Elite TD Football Cle\n",
            "3 : \n",
            "4 : 129.99\n",
            "5 : http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat\n",
            "9,2,Nike Adult Vapor Jet 3.0 Receiver Gloves,,50.0,http://images.acmesports.sports/Nike+Adult+Vapor+Jet+3.0+Receiver+Gloves\n",
            "0 : 9\n",
            "1 : 2\n",
            "2 : Nike Adult Vapor Jet 3.0 Receiver Gloves\n",
            "3 : \n",
            "4 : 50.0\n",
            "5 : http://images.acmesports.sports/Nike+Adult+Vapor+Jet+3.0+Receiver+Gloves\n",
            "10,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n",
            "0 : 10\n",
            "1 : 2\n",
            "2 : Under Armour Men's Highlight MC Football Clea\n",
            "3 : \n",
            "4 : 129.99\n",
            "5 : http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQmstjLRZSX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isInt(s):\n",
        "    try: \n",
        "        int(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n6Fc3oxZ8iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isValidP(p):\n",
        "  try:\n",
        "    product_id=int(p.split(\",\")[0])\n",
        "    product_category_id=int(p.split(\",\")[1])\n",
        "    product_name=\"\".join(p.split(\",\")[2:-2])\n",
        "    # product_description= p.split(\",\")[3]\n",
        "    product_price=float(p.split(\",\")[-2])\n",
        "    return True\n",
        "  except ValueError:\n",
        "      return None"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqHqp5nOc0Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just in case some special situations, such as the following\n",
        "#\n",
        "# 685,31,\"TaylorMade SLDR Irons - (Steel) 4-PW, AW\",,899.99,http://images.acmesports.sports/TaylorMade+SLDR+Irons+-+%28Steel%29+4-PW%2C+AW\n",
        "#\n",
        "# Where a comma in a string, which will cause a lot of troubles, which are difficult to find.\n",
        "\n",
        "def safe2Product(p):\n",
        "  try:\n",
        "    product_id=int(p.split(\",\")[0])\n",
        "    product_category_id=int(p.split(\",\")[1])\n",
        "    product_name=\"\".join(p.split(\",\")[2:-2])\n",
        "    # product_description= p.split(\",\")[3]\n",
        "    product_price=float(p.split(\",\")[-2])\n",
        "    return Product(product_id=product_id, product_category_id=product_category_id,product_name=product_name, product_description=\"\",product_price=product_price )\n",
        "  except ValueError:\n",
        "      return None"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thlhxk_-f-j7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5077d3c4-d8ce-43cc-8a5c-f05fd2278acb"
      },
      "source": [
        "for p in products_txt.take(10):\n",
        "    product_id=int(p.split(\",\")[0])\n",
        "    product_category_id=int(p.split(\",\")[1])\n",
        "    product_name=\"\".join(p.split(\",\")[2:-2])\n",
        "    # product_description= p.split(\",\")[3]\n",
        "    product_price=float(p.split(\",\")[-2])\n",
        "    print(product_id,product_category_id,product_name, product_price)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2 Quest Q64 10 FT. x 10 FT. Slant Leg Instant U 59.98\n",
            "2 2 Under Armour Men's Highlight MC Football Clea 129.99\n",
            "3 2 Under Armour Men's Renegade D Mid Football Cl 89.99\n",
            "4 2 Under Armour Men's Renegade D Mid Football Cl 89.99\n",
            "5 2 Riddell Youth Revolution Speed Custom Footbal 199.99\n",
            "6 2 Jordan Men's VI Retro TD Football Cleat 134.99\n",
            "7 2 Schutt Youth Recruit Hybrid Custom Football H 99.99\n",
            "8 2 Nike Men's Vapor Carbon Elite TD Football Cle 129.99\n",
            "9 2 Nike Adult Vapor Jet 3.0 Receiver Gloves 50.0\n",
            "10 2 Under Armour Men's Highlight MC Football Clea 129.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62rZ2-XdZZsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "products_invalid=products_txt.filter(lambda p: not isValidP(p))\n",
        "for i in products_invalid.take(10):\n",
        "  print(i)\n",
        "  for j in enumerate(i.split(\",\")):\n",
        "   print(j[0],\":\", j[1])\n",
        "# print_top10(products_invalid)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLRd9BeB1mbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Product=namedtuple(\"Product\", \"product_id product_category_id product_name product_description product_price\")"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahXyyZ793CSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "products=products_txt.map(lambda p: safe2Product(p))"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDw-5Urhiw7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f8487339-0c3f-4fb4-a2ff-1d3c61c337ca"
      },
      "source": [
        "products.count()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1345"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrG2EFxO6CpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "17070a90-f06c-4464-8987-6fa96b29347a"
      },
      "source": [
        "print_top10(products.filter(lambda p: p.product_id==975))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Product(product_id=975, product_category_id=44, product_name='Eureka! Tetragon 5 Five Person Tent', product_description='', product_price=129.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ABljEJO7BCS"
      },
      "source": [
        "## Data Preparation - Orders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3XRxTuOL7BCU",
        "colab": {}
      },
      "source": [
        "orders_txt=sc.textFile(\"././retail_db/orders/part-00000\")"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5I3hKiCZ7BCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1f728328-07fb-4a8a-a3f6-ad29ce0e2066"
      },
      "source": [
        "from datetime import datetime\n",
        "for i in orders_txt.take(10):\n",
        "  print(i)\n",
        "  for j in enumerate(i.split(\",\")):\n",
        "    if (j[0]==1):\n",
        "      print(j[0],\":\", datetime.strptime(j[1][:-2], \"%Y-%m-%d %H:%M:%S\"))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
            "1 : 2013-07-25 00:00:00\n",
            "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
            "1 : 2013-07-25 00:00:00\n",
            "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
            "1 : 2013-07-25 00:00:00\n",
            "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
            "1 : 2013-07-25 00:00:00\n",
            "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
            "1 : 2013-07-25 00:00:00\n",
            "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
            "1 : 2013-07-25 00:00:00\n",
            "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
            "1 : 2013-07-25 00:00:00\n",
            "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
            "1 : 2013-07-25 00:00:00\n",
            "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
            "1 : 2013-07-25 00:00:00\n",
            "10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT\n",
            "1 : 2013-07-25 00:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vRLLGe7i7BCb",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "Order=namedtuple(\"Order\", \"order_id order_date order_customer_id order_status\")"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SscBeAMo7BCf",
        "colab": {}
      },
      "source": [
        "orders=orders_txt.map(lambda p: Order(order_id=int(p.split(\",\")[0]), order_date=datetime.strptime(p.split(\",\")[1][:-2], \"%Y-%m-%d %H:%M:%S\"),\\\n",
        "                  order_customer_id=int(p.split(\",\")[2]),\\\n",
        "                  order_status= p.split(\",\")[3]))"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8qC0JP3g7BCh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c3916f65-bb9d-4eda-ec6e-6da4685e43c3"
      },
      "source": [
        "print_top10(orders)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order(order_id=1, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11599, order_status='CLOSED')\n",
            "Order(order_id=2, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=256, order_status='PENDING_PAYMENT')\n",
            "Order(order_id=3, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=12111, order_status='COMPLETE')\n",
            "Order(order_id=4, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=8827, order_status='CLOSED')\n",
            "Order(order_id=5, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11318, order_status='COMPLETE')\n",
            "Order(order_id=6, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=7130, order_status='COMPLETE')\n",
            "Order(order_id=7, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=4530, order_status='COMPLETE')\n",
            "Order(order_id=8, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=2911, order_status='PROCESSING')\n",
            "Order(order_id=9, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=5657, order_status='PENDING_PAYMENT')\n",
            "Order(order_id=10, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=5648, order_status='PENDING_PAYMENT')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hDPashjyAj30"
      },
      "source": [
        "## Data Preparation - Order_Items\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cd5IgY9rAj32",
        "colab": {}
      },
      "source": [
        "order_Items_txt=sc.textFile(\"././retail_db/order_items/part-00000\")"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "331MCSTCAj35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "56def8d7-5118-49e4-eb78-dcb98be1a3e6"
      },
      "source": [
        "for i in order_Items_txt.take(2):\n",
        "  print(i)\n",
        "  for j in enumerate(i.split(\",\")):\n",
        "   print(j[0],\":\", j[1])"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,1,957,1,299.98,299.98\n",
            "0 : 1\n",
            "1 : 1\n",
            "2 : 957\n",
            "3 : 1\n",
            "4 : 299.98\n",
            "5 : 299.98\n",
            "2,2,1073,1,199.99,199.99\n",
            "0 : 2\n",
            "1 : 2\n",
            "2 : 1073\n",
            "3 : 1\n",
            "4 : 199.99\n",
            "5 : 199.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xMxd1_VAj39",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Order_Items=namedtuple(\"Order_Items\", \"order_item_id order_item_order_id order_item_product_id order_item_quantity order_item_subtotal order_item_product_price\")"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fK5qZvuuAj4A",
        "colab": {}
      },
      "source": [
        "order_Items=order_Items_txt.map(lambda p: Order_Items(order_item_id=int(p.split(\",\")[0]), order_item_order_id=int(p.split(\",\")[1]),\\\n",
        "                  order_item_product_id=int(p.split(\",\")[2]),order_item_quantity=int(p.split(\",\")[3]), \\\n",
        "                  order_item_subtotal=float(p.split(\",\")[4]),\\\n",
        "                  order_item_product_price=float(p.split(\",\")[5])))\n"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeQonWHkAj4C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "9ef72a43-0444-446d-ed83-1dfd3aab1398"
      },
      "source": [
        "print_top10(order_Items)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order_Items(order_item_id=1, order_item_order_id=1, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=2, order_item_order_id=2, order_item_product_id=1073, order_item_quantity=1, order_item_subtotal=199.99, order_item_product_price=199.99)\n",
            "Order_Items(order_item_id=3, order_item_order_id=2, order_item_product_id=502, order_item_quantity=5, order_item_subtotal=250.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=4, order_item_order_id=2, order_item_product_id=403, order_item_quantity=1, order_item_subtotal=129.99, order_item_product_price=129.99)\n",
            "Order_Items(order_item_id=5, order_item_order_id=4, order_item_product_id=897, order_item_quantity=2, order_item_subtotal=49.98, order_item_product_price=24.99)\n",
            "Order_Items(order_item_id=6, order_item_order_id=4, order_item_product_id=365, order_item_quantity=5, order_item_subtotal=299.95, order_item_product_price=59.99)\n",
            "Order_Items(order_item_id=7, order_item_order_id=4, order_item_product_id=502, order_item_quantity=3, order_item_subtotal=150.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=8, order_item_order_id=4, order_item_product_id=1014, order_item_quantity=4, order_item_subtotal=199.92, order_item_product_price=49.98)\n",
            "Order_Items(order_item_id=9, order_item_order_id=5, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=10, order_item_order_id=5, order_item_product_id=365, order_item_quantity=5, order_item_subtotal=299.95, order_item_product_price=59.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTEIA2NxHoi_",
        "colab_type": "text"
      },
      "source": [
        "# Now we can practice Spark -- SQL\n",
        "\n",
        "**Spark SQL is the primary topic covered by CCA Spark and Hadoop Developer Exam (CCA175).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADWNgv5PmIPL",
        "colab_type": "text"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59v7eMU0fsZA",
        "colab_type": "text"
      },
      "source": [
        "At first, we create dataframe\n",
        "\n",
        "**There is NO header here!**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urK-4w_M6Fvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "40527b07-edc6-482e-8330-2720db9dfbd4"
      },
      "source": [
        "print_top10(products_txt)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy\n",
            "2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n",
            "3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
            "4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
            "5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet\n",
            "6,2,Jordan Men's VI Retro TD Football Cleat,,134.99,http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat\n",
            "7,2,Schutt Youth Recruit Hybrid Custom Football H,,99.99,http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014\n",
            "8,2,Nike Men's Vapor Carbon Elite TD Football Cle,,129.99,http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat\n",
            "9,2,Nike Adult Vapor Jet 3.0 Receiver Gloves,,50.0,http://images.acmesports.sports/Nike+Adult+Vapor+Jet+3.0+Receiver+Gloves\n",
            "10,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnuw-Uwf5I2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "137dd9f2-1429-4a81-fd83-faabcc7e4b4c"
      },
      "source": [
        "products_df=spark.read.csv(\"retail_db/products\",sep=\",\", header=False, inferSchema=True)\n",
        "\n",
        "products_df=products_df.toDF(\"product_id\", \"product_category_id\",\"product_name\", \"product_description\",\"product_price\",\"product_image\")\n",
        "products_df.show()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|         1|                  2|Quest Q64 10 FT. ...|               null|        59.98|http://images.acm...|\n",
            "|         2|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "|         3|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|         4|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|         5|                  2|Riddell Youth Rev...|               null|       199.99|http://images.acm...|\n",
            "|         6|                  2|Jordan Men's VI R...|               null|       134.99|http://images.acm...|\n",
            "|         7|                  2|Schutt Youth Recr...|               null|        99.99|http://images.acm...|\n",
            "|         8|                  2|Nike Men's Vapor ...|               null|       129.99|http://images.acm...|\n",
            "|         9|                  2|Nike Adult Vapor ...|               null|         50.0|http://images.acm...|\n",
            "|        10|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "|        11|                  2|Fitness Gear 300 ...|               null|       209.99|http://images.acm...|\n",
            "|        12|                  2|Under Armour Men'...|               null|       139.99|http://images.acm...|\n",
            "|        13|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|        14|                  2|Quik Shade Summit...|               null|       199.99|http://images.acm...|\n",
            "|        15|                  2|Under Armour Kids...|               null|        59.99|http://images.acm...|\n",
            "|        16|                  2|Riddell Youth 360...|               null|       299.99|http://images.acm...|\n",
            "|        17|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "|        18|                  2|Reebok Men's Full...|               null|        29.97|http://images.acm...|\n",
            "|        19|                  2|Nike Men's Finger...|               null|       124.99|http://images.acm...|\n",
            "|        20|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHYkzi0BjO3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "ce47b5da-13b6-4794-fdd6-21be38fee4db"
      },
      "source": [
        "products_df.select(\"product_id\",\"product_name\").show()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|        product_name|\n",
            "+----------+--------------------+\n",
            "|         1|Quest Q64 10 FT. ...|\n",
            "|         2|Under Armour Men'...|\n",
            "|         3|Under Armour Men'...|\n",
            "|         4|Under Armour Men'...|\n",
            "|         5|Riddell Youth Rev...|\n",
            "|         6|Jordan Men's VI R...|\n",
            "|         7|Schutt Youth Recr...|\n",
            "|         8|Nike Men's Vapor ...|\n",
            "|         9|Nike Adult Vapor ...|\n",
            "|        10|Under Armour Men'...|\n",
            "|        11|Fitness Gear 300 ...|\n",
            "|        12|Under Armour Men'...|\n",
            "|        13|Under Armour Men'...|\n",
            "|        14|Quik Shade Summit...|\n",
            "|        15|Under Armour Kids...|\n",
            "|        16|Riddell Youth 360...|\n",
            "|        17|Under Armour Men'...|\n",
            "|        18|Reebok Men's Full...|\n",
            "|        19|Nike Men's Finger...|\n",
            "|        20|Under Armour Men'...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO2xmvlKjwlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "b122dcc8-5864-4f9d-869e-3718fcd76d17"
      },
      "source": [
        "products_df.createOrReplaceTempView(\"products\")\n",
        "\n",
        "spark.sql(\"select * from Products\").show() # Not case sensitive in view Name and sql keyword"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|         1|                  2|Quest Q64 10 FT. ...|               null|        59.98|http://images.acm...|\n",
            "|         2|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "|         3|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|         4|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|         5|                  2|Riddell Youth Rev...|               null|       199.99|http://images.acm...|\n",
            "|         6|                  2|Jordan Men's VI R...|               null|       134.99|http://images.acm...|\n",
            "|         7|                  2|Schutt Youth Recr...|               null|        99.99|http://images.acm...|\n",
            "|         8|                  2|Nike Men's Vapor ...|               null|       129.99|http://images.acm...|\n",
            "|         9|                  2|Nike Adult Vapor ...|               null|         50.0|http://images.acm...|\n",
            "|        10|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "|        11|                  2|Fitness Gear 300 ...|               null|       209.99|http://images.acm...|\n",
            "|        12|                  2|Under Armour Men'...|               null|       139.99|http://images.acm...|\n",
            "|        13|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|        14|                  2|Quik Shade Summit...|               null|       199.99|http://images.acm...|\n",
            "|        15|                  2|Under Armour Kids...|               null|        59.99|http://images.acm...|\n",
            "|        16|                  2|Riddell Youth 360...|               null|       299.99|http://images.acm...|\n",
            "|        17|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "|        18|                  2|Reebok Men's Full...|               null|        29.97|http://images.acm...|\n",
            "|        19|                  2|Nike Men's Finger...|               null|       124.99|http://images.acm...|\n",
            "|        20|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Fwceh6kgz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "cbf2aa37-6bad-4ba9-d17f-4d70cc33dceb"
      },
      "source": [
        "spark.sql(\" select * from products where Product_Price<100\").show() # Column Name is not case sensitive"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|         1|                  2|Quest Q64 10 FT. ...|               null|        59.98|http://images.acm...|\n",
            "|         3|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|         4|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|         7|                  2|Schutt Youth Recr...|               null|        99.99|http://images.acm...|\n",
            "|         9|                  2|Nike Adult Vapor ...|               null|         50.0|http://images.acm...|\n",
            "|        13|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
            "|        15|                  2|Under Armour Kids...|               null|        59.99|http://images.acm...|\n",
            "|        18|                  2|Reebok Men's Full...|               null|        29.97|http://images.acm...|\n",
            "|        21|                  2|Under Armour Kids...|               null|        54.99|http://images.acm...|\n",
            "|        22|                  2|Kijaro Dual Lock ...|               null|        29.99|http://images.acm...|\n",
            "|        24|                  2|Elevation Trainin...|               null|        79.99|http://images.acm...|\n",
            "|        25|                  3|Quest Q64 10 FT. ...|               null|        59.98|http://images.acm...|\n",
            "|        26|                  3|Nike Men's USA Wh...|               null|         90.0|http://images.acm...|\n",
            "|        27|                  3|Nike Youth USA Aw...|               null|         75.0|http://images.acm...|\n",
            "|        28|                  3|adidas Brazuca 20...|               null|        29.99|http://images.acm...|\n",
            "|        29|                  3|Nike Men's USA Aw...|               null|         90.0|http://images.acm...|\n",
            "|        30|                  3|adidas Men's Germ...|               null|         90.0|http://images.acm...|\n",
            "|        31|                  3|   Nike+ Fuelband SE|               null|         99.0|http://images.acm...|\n",
            "|        33|                  3|adidas Brazuca 20...|               null|        39.99|http://images.acm...|\n",
            "|        34|                  3|\"Nike Women's Pro...|               null|         28.0|http://images.acm...|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4hV_O6lGg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "a749ac33-78f7-4946-86c8-2094a84e52cc"
      },
      "source": [
        "spark.sql(\"Select Product_id, length(Product_Name) from products\").show() # length function!"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|Product_id|length(Product_Name)|\n",
            "+----------+--------------------+\n",
            "|         1|                  45|\n",
            "|         2|                  45|\n",
            "|         3|                  45|\n",
            "|         4|                  45|\n",
            "|         5|                  45|\n",
            "|         6|                  39|\n",
            "|         7|                  45|\n",
            "|         8|                  45|\n",
            "|         9|                  40|\n",
            "|        10|                  45|\n",
            "|        11|                  38|\n",
            "|        12|                  45|\n",
            "|        13|                  45|\n",
            "|        14|                  45|\n",
            "|        15|                  45|\n",
            "|        16|                  40|\n",
            "|        17|                  45|\n",
            "|        18|                  37|\n",
            "|        19|                  39|\n",
            "|        20|                  45|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XS0KrxeSmSrj",
        "colab": {}
      },
      "source": [
        "sql1=spark.sql(\" select * from products where Product_Price<100\")"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtOUROPAmdkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "95fc43a0-3f52-4ea5-dc36-ac53f65c54ae"
      },
      "source": [
        "#dataframe to rdd\n",
        "\n",
        "for i in sql1.rdd.map(lambda p: p.product_name).take(10): # column IS case sensitive \n",
        "  print(i)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quest Q64 10 FT. x 10 FT. Slant Leg Instant U\n",
            "Under Armour Men's Renegade D Mid Football Cl\n",
            "Under Armour Men's Renegade D Mid Football Cl\n",
            "Schutt Youth Recruit Hybrid Custom Football H\n",
            "Nike Adult Vapor Jet 3.0 Receiver Gloves\n",
            "Under Armour Men's Renegade D Mid Football Cl\n",
            "Under Armour Kids' Highlight RM Alter Ego Sup\n",
            "Reebok Men's Full Zip Training Jacket\n",
            "Under Armour Kids' Highlight RM Football Clea\n",
            "Kijaro Dual Lock Chair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlG1vRgQ8-S2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "595efa0b-64b1-4004-8836-d417fbce0030"
      },
      "source": [
        "orders_df=spark.read.csv(\"retail_db/orders\", sep=\",\", header=False, inferSchema=True)\n",
        "orders_df=orders_df.toDF(\"order_customer_id\",\"order_date\",\"order_id\",\"order_status\")\n",
        "orders_df.show()"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+--------+---------------+\n",
            "|order_customer_id|         order_date|order_id|   order_status|\n",
            "+-----------------+-------------------+--------+---------------+\n",
            "|                1|2013-07-25 00:00:00|   11599|         CLOSED|\n",
            "|                2|2013-07-25 00:00:00|     256|PENDING_PAYMENT|\n",
            "|                3|2013-07-25 00:00:00|   12111|       COMPLETE|\n",
            "|                4|2013-07-25 00:00:00|    8827|         CLOSED|\n",
            "|                5|2013-07-25 00:00:00|   11318|       COMPLETE|\n",
            "|                6|2013-07-25 00:00:00|    7130|       COMPLETE|\n",
            "|                7|2013-07-25 00:00:00|    4530|       COMPLETE|\n",
            "|                8|2013-07-25 00:00:00|    2911|     PROCESSING|\n",
            "|                9|2013-07-25 00:00:00|    5657|PENDING_PAYMENT|\n",
            "|               10|2013-07-25 00:00:00|    5648|PENDING_PAYMENT|\n",
            "|               11|2013-07-25 00:00:00|     918| PAYMENT_REVIEW|\n",
            "|               12|2013-07-25 00:00:00|    1837|         CLOSED|\n",
            "|               13|2013-07-25 00:00:00|    9149|PENDING_PAYMENT|\n",
            "|               14|2013-07-25 00:00:00|    9842|     PROCESSING|\n",
            "|               15|2013-07-25 00:00:00|    2568|       COMPLETE|\n",
            "|               16|2013-07-25 00:00:00|    7276|PENDING_PAYMENT|\n",
            "|               17|2013-07-25 00:00:00|    2667|       COMPLETE|\n",
            "|               18|2013-07-25 00:00:00|    1205|         CLOSED|\n",
            "|               19|2013-07-25 00:00:00|    9488|PENDING_PAYMENT|\n",
            "|               20|2013-07-25 00:00:00|    9198|     PROCESSING|\n",
            "+-----------------+-------------------+--------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXCaQQhFnbJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "4b66b387-4901-41b3-fbef-a1e2b5b6253d"
      },
      "source": [
        "orders_df.createOrReplaceTempView(\"orders\")\n",
        "spark.sql(\"select * from orders\").show()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+--------+---------------+\n",
            "|order_customer_id|         order_date|order_id|   order_status|\n",
            "+-----------------+-------------------+--------+---------------+\n",
            "|                1|2013-07-25 00:00:00|   11599|         CLOSED|\n",
            "|                2|2013-07-25 00:00:00|     256|PENDING_PAYMENT|\n",
            "|                3|2013-07-25 00:00:00|   12111|       COMPLETE|\n",
            "|                4|2013-07-25 00:00:00|    8827|         CLOSED|\n",
            "|                5|2013-07-25 00:00:00|   11318|       COMPLETE|\n",
            "|                6|2013-07-25 00:00:00|    7130|       COMPLETE|\n",
            "|                7|2013-07-25 00:00:00|    4530|       COMPLETE|\n",
            "|                8|2013-07-25 00:00:00|    2911|     PROCESSING|\n",
            "|                9|2013-07-25 00:00:00|    5657|PENDING_PAYMENT|\n",
            "|               10|2013-07-25 00:00:00|    5648|PENDING_PAYMENT|\n",
            "|               11|2013-07-25 00:00:00|     918| PAYMENT_REVIEW|\n",
            "|               12|2013-07-25 00:00:00|    1837|         CLOSED|\n",
            "|               13|2013-07-25 00:00:00|    9149|PENDING_PAYMENT|\n",
            "|               14|2013-07-25 00:00:00|    9842|     PROCESSING|\n",
            "|               15|2013-07-25 00:00:00|    2568|       COMPLETE|\n",
            "|               16|2013-07-25 00:00:00|    7276|PENDING_PAYMENT|\n",
            "|               17|2013-07-25 00:00:00|    2667|       COMPLETE|\n",
            "|               18|2013-07-25 00:00:00|    1205|         CLOSED|\n",
            "|               19|2013-07-25 00:00:00|    9488|PENDING_PAYMENT|\n",
            "|               20|2013-07-25 00:00:00|    9198|     PROCESSING|\n",
            "+-----------------+-------------------+--------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb42X2LT9nf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "e8bd0ca0-3c09-4e27-ce53-2214faaa0a7e"
      },
      "source": [
        "order_item_df=spark.read.csv(\"retail_db/order_items\", sep=\",\", header=False, inferSchema=True)\n",
        "order_item_df=order_item_df.toDF(\"order_item_id\",\"order_item_order_id\",\"order_item_product_id\",\"order_item_product_price\",\"order_item_quantity\",\"order_item_subtotal\")\n",
        "order_item_df.show()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
            "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
            "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
            "|            1|                  1|                  957|                       1|             299.98|             299.98|\n",
            "|            2|                  2|                 1073|                       1|             199.99|             199.99|\n",
            "|            3|                  2|                  502|                       5|              250.0|               50.0|\n",
            "|            4|                  2|                  403|                       1|             129.99|             129.99|\n",
            "|            5|                  4|                  897|                       2|              49.98|              24.99|\n",
            "|            6|                  4|                  365|                       5|             299.95|              59.99|\n",
            "|            7|                  4|                  502|                       3|              150.0|               50.0|\n",
            "|            8|                  4|                 1014|                       4|             199.92|              49.98|\n",
            "|            9|                  5|                  957|                       1|             299.98|             299.98|\n",
            "|           10|                  5|                  365|                       5|             299.95|              59.99|\n",
            "|           11|                  5|                 1014|                       2|              99.96|              49.98|\n",
            "|           12|                  5|                  957|                       1|             299.98|             299.98|\n",
            "|           13|                  5|                  403|                       1|             129.99|             129.99|\n",
            "|           14|                  7|                 1073|                       1|             199.99|             199.99|\n",
            "|           15|                  7|                  957|                       1|             299.98|             299.98|\n",
            "|           16|                  7|                  926|                       5|              79.95|              15.99|\n",
            "|           17|                  8|                  365|                       3|             179.97|              59.99|\n",
            "|           18|                  8|                  365|                       5|             299.95|              59.99|\n",
            "|           19|                  8|                 1014|                       4|             199.92|              49.98|\n",
            "|           20|                  8|                  502|                       1|               50.0|               50.0|\n",
            "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEXD09f4o8uP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "e5276297-e771-4a49-8351-61f4ddaa781e"
      },
      "source": [
        "order_item_df.createOrReplaceTempView(\"order_item\")\n",
        "\n",
        "spark.sql(\"select * from order_item\").show()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
            "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
            "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
            "|            1|                  1|                  957|                       1|             299.98|             299.98|\n",
            "|            2|                  2|                 1073|                       1|             199.99|             199.99|\n",
            "|            3|                  2|                  502|                       5|              250.0|               50.0|\n",
            "|            4|                  2|                  403|                       1|             129.99|             129.99|\n",
            "|            5|                  4|                  897|                       2|              49.98|              24.99|\n",
            "|            6|                  4|                  365|                       5|             299.95|              59.99|\n",
            "|            7|                  4|                  502|                       3|              150.0|               50.0|\n",
            "|            8|                  4|                 1014|                       4|             199.92|              49.98|\n",
            "|            9|                  5|                  957|                       1|             299.98|             299.98|\n",
            "|           10|                  5|                  365|                       5|             299.95|              59.99|\n",
            "|           11|                  5|                 1014|                       2|              99.96|              49.98|\n",
            "|           12|                  5|                  957|                       1|             299.98|             299.98|\n",
            "|           13|                  5|                  403|                       1|             129.99|             129.99|\n",
            "|           14|                  7|                 1073|                       1|             199.99|             199.99|\n",
            "|           15|                  7|                  957|                       1|             299.98|             299.98|\n",
            "|           16|                  7|                  926|                       5|              79.95|              15.99|\n",
            "|           17|                  8|                  365|                       3|             179.97|              59.99|\n",
            "|           18|                  8|                  365|                       5|             299.95|              59.99|\n",
            "|           19|                  8|                 1014|                       4|             199.92|              49.98|\n",
            "|           20|                  8|                  502|                       1|               50.0|               50.0|\n",
            "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k4vz6yspqTH",
        "colab_type": "text"
      },
      "source": [
        "## Join (as, alias work!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a363PI2AtmxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3af48319-80e4-46d5-f733-2207b2d9579c"
      },
      "source": [
        "# Vow! we can use SQL join directly\n",
        "spark.sql(\"select  i.order_item_order_id as order_id, i.order_item_product_id as product_id, p.product_name from order_item i, products p where i.order_item_product_id=p.product_id and i.order_item_order_id=1 \").show()"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+--------------------+\n",
            "|order_id|product_id|        product_name|\n",
            "+--------+----------+--------------------+\n",
            "|       1|       957|Diamondback Women...|\n",
            "+--------+----------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AITpmGTG86rk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ed2c1f8d-9be9-4b81-e381-adbffbfa8221"
      },
      "source": [
        "# Vow! we can use SQL join directly\n",
        "spark.sql(\"select  i.order_item_order_id as order_id, i.order_item_product_id as product_id, p.product_name from order_item i JOIN products p on i.order_item_product_id=p.product_id where i.order_item_order_id=1 \").show()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+--------------------+\n",
            "|order_id|product_id|        product_name|\n",
            "+--------+----------+--------------------+\n",
            "|       1|       957|Diamondback Women...|\n",
            "+--------+----------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_7k_Bm-A_ZN",
        "colab_type": "text"
      },
      "source": [
        "## Aggregate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUIObno2BC75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "9b2bba2a-9ce4-4a86-e615-b05050b93081"
      },
      "source": [
        "spark.sql(\"select  i.order_item_order_id as order_id, count(1) as count_items from order_item i group by order_item_order_id\").show()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------+\n",
            "|order_id|count_items|\n",
            "+--------+-----------+\n",
            "|     148|          3|\n",
            "|     463|          4|\n",
            "|     471|          2|\n",
            "|     496|          5|\n",
            "|    1088|          2|\n",
            "|    1580|          1|\n",
            "|    1591|          3|\n",
            "|    1645|          5|\n",
            "|    2366|          1|\n",
            "|    2659|          5|\n",
            "|    2866|          4|\n",
            "|    3175|          2|\n",
            "|    3749|          1|\n",
            "|    3794|          1|\n",
            "|    3918|          4|\n",
            "|    3997|          2|\n",
            "|    4101|          1|\n",
            "|    4519|          1|\n",
            "|    4818|          1|\n",
            "|    4900|          2|\n",
            "+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oma9hmQ4CgqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql4=spark.sql(\"select  i.order_item_order_id as order_id, count(*) as count_items from order_item i group by order_item_order_id\")"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrS1X4csCmmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql4.createOrReplaceTempView(\"i_by_o\")"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh6KUEfNDekU",
        "colab_type": "text"
      },
      "source": [
        "### Query on query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aji4NOScB8dU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "082b9e33-9aba-4d50-c72a-22675d7d1ee2"
      },
      "source": [
        "# Vow! we can use SQL join directly\n",
        "spark.sql(\"select  * from i_by_o i JOIN orders o on i.order_id=o.order_id where o.order_id=2\").show()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------+-----------------+-------------------+--------+---------------+\n",
            "|order_id|count_items|order_customer_id|         order_date|order_id|   order_status|\n",
            "+--------+-----------+-----------------+-------------------+--------+---------------+\n",
            "|       2|          3|            15192|2013-10-29 00:00:00|       2|PENDING_PAYMENT|\n",
            "|       2|          3|            33865|2014-02-18 00:00:00|       2|       COMPLETE|\n",
            "|       2|          3|            57963|2013-08-02 00:00:00|       2|        ON_HOLD|\n",
            "|       2|          3|            67863|2013-11-30 00:00:00|       2|       COMPLETE|\n",
            "+--------+-----------+-----------------+-------------------+--------+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNu-IaIBvj0H",
        "colab_type": "text"
      },
      "source": [
        "## Save/ Read\n",
        "**We use SAME options for save and read**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0JdRsSHvm1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql_test2=spark.sql(\"select  i.order_item_order_id as order_id, i.order_item_product_id as product_id, p.product_name from order_item i JOIN products p on i.order_item_product_id=p.product_id where i.order_item_order_id=1 \")"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIelJFgJ-7H-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql_test2.coalesce(1).write.options(compression=\"gzip\",header=True,sep=\"#\").csv(\"sql_test5.csv\") # coalesce(1) is important to put all contents in 1 file."
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3edpTiSAGBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "008c4c79-b4a1-4b94-944f-a5c1c1e19b15"
      },
      "source": [
        "spark.read.options(compression=\"gzip\",header=True,sep=\"#\").csv(\"sql_test5.csv\").show() # compression in options function!"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+--------------------+\n",
            "|order_id|product_id|        product_name|\n",
            "+--------+----------+--------------------+\n",
            "|       1|       957|Diamondback Women...|\n",
            "+--------+----------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuFfLyFDBpRh",
        "colab_type": "text"
      },
      "source": [
        "### Order by asc/desc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ZgQfaEAqs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "32be6e68-9422-4e44-9b91-d6dd9eefc61a"
      },
      "source": [
        "spark.sql(\"select * from products order by product_price desc\").show()"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "|       208|                 10| SOLE E35 Elliptical|               null|      1999.99|http://images.acm...|\n",
            "|       199|                 10|  SOLE F85 Treadmill|               null|      1799.99|http://images.acm...|\n",
            "|        66|                  4|  SOLE F85 Treadmill|               null|      1799.99|http://images.acm...|\n",
            "|       496|                 22|  SOLE F85 Treadmill|               null|      1799.99|http://images.acm...|\n",
            "|      1048|                 47|\"Spalding Beast 6...|               null|      1099.99|http://images.acm...|\n",
            "|        60|                  4| SOLE E25 Elliptical|               null|       999.99|http://images.acm...|\n",
            "|       694|                 32|Callaway Women's ...|               null|       999.99|http://images.acm...|\n",
            "|       197|                 10| SOLE E25 Elliptical|               null|       999.99|http://images.acm...|\n",
            "|       488|                 22| SOLE E25 Elliptical|               null|       999.99|http://images.acm...|\n",
            "|       695|                 32|Callaway Women's ...|               null|       999.99|http://images.acm...|\n",
            "|       685|                 31|TaylorMade SLDR I...|               null|       899.99|http://images.acm...|\n",
            "|       676|                 31|PING G30 Irons - ...|               null|        899.0|http://images.acm...|\n",
            "|       229|                 11|Marcy Diamond 901...|               null|       799.99|http://images.acm...|\n",
            "|      1069|                 48|\"Tiga 11'4\"\" Stan...|               null|       799.99|http://images.acm...|\n",
            "|       675|                 31|PING G30 Irons - ...|               null|        799.0|http://images.acm...|\n",
            "|       698|                 32|Cleveland Women's...|               null|       699.99|http://images.acm...|\n",
            "|      1054|                 47|\"Spalding NBA 54\"...|               null|       699.99|http://images.acm...|\n",
            "|       226|                 11|Bowflex SelectTec...|               null|       599.99|http://images.acm...|\n",
            "|       689|                 31|Tour Edge XCG7 Ir...|               null|       599.99|http://images.acm...|\n",
            "|       690|                 31|Tour Edge XCG7 X-...|               null|       599.99|http://images.acm...|\n",
            "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC_hadLRPa4Y",
        "colab_type": "text"
      },
      "source": [
        "# CCA 175 Exam Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us-is-NaPfZw",
        "colab_type": "text"
      },
      "source": [
        "## Common imports\n",
        "\n",
        "```Python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "from pyspark.sql import Row\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ybNPVRbPwkn",
        "colab_type": "text"
      },
      "source": [
        "## Common Commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lPg1r6TP1Bg",
        "colab_type": "text"
      },
      "source": [
        "### sqoop\n",
        "\n",
        "sqoop import --conect jdbc://gateway/problem1 --table customer --username cloudera --password cloudera --target-dir /user/cert/problem1/solution/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVz0yfX2P4nI",
        "colab_type": "text"
      },
      "source": [
        "### pyspark\n",
        "\n",
        "![alt text](https://img2018.cnblogs.com/blog/724315/201809/724315-20180921223109361-1426293188.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHUgSAHP8yw",
        "colab_type": "text"
      },
      "source": [
        "### spark-submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD4pWEi4QC0I",
        "colab_type": "text"
      },
      "source": [
        "### hdfs dfs \n",
        "\n",
        "![alt text](https://linoxide.com/images/hadoop-hdfs-commands-cheatsheet-900x1500.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIDPI9oGQGof",
        "colab_type": "text"
      },
      "source": [
        "### impala-shell\n",
        "\n",
        "impala-shell -i worker1\n",
        "\n",
        "create database problem3;\n",
        "\n",
        "use problem3;\n",
        "\n",
        "create external table solution (\n",
        "id int,\n",
        "fname string,\n",
        ")\n",
        "row format delimited\n",
        "fields terminated by ‘\\t’\n",
        "location ‘/user…’\n",
        "\n",
        "\n",
        "\n",
        "CREATE [EXTERNAL] TABLE [IF NOT EXISTS] db_name.]table_name\n",
        "  [COMMENT 'table_comment']\n",
        "  [WITH SERDEPROPERTIES ('key1'='value1', 'key2'='value2', ...)]\n",
        "  [\n",
        "   [ROW FORMAT row_format] [STORED AS ctas_file_format]\n",
        "  ]\n",
        "  [LOCATION 'hdfs_path']\n",
        "  [TBLPROPERTIES ('key1'='value1', 'key2'='value2', ...)]\n",
        "  [CACHED IN 'pool_name' [WITH REPLICATION = integer] | UNCACHED]\n",
        "AS\n",
        "  select_statement\n",
        "\n",
        "primitive_type:\n",
        "    TINYINT\n",
        "  | SMALLINT\n",
        "  | INT\n",
        "  | BIGINT\n",
        "  | BOOLEAN\n",
        "  | FLOAT\n",
        "  | DOUBLE\n",
        "  | DECIMAL\n",
        "  | STRING\n",
        "  | CHAR\n",
        "  | VARCHAR\n",
        "  | TIMESTAMP\n",
        "\n",
        "complex_type:\n",
        "    struct_type\n",
        "  | array_type\n",
        "  | map_type\n",
        "\n",
        "struct_type: STRUCT < name : primitive_or_complex_type [COMMENT 'comment_string'], ... >\n",
        "\n",
        "array_type: ARRAY < primitive_or_complex_type >\n",
        "\n",
        "map_type: MAP < primitive_type, primitive_or_complex_type >\n",
        "\n",
        "row_format:\n",
        "  DELIMITED [FIELDS TERMINATED BY 'char' [ESCAPED BY 'char']]\n",
        "  [LINES TERMINATED BY 'char']\n",
        "\n",
        "file_format:\n",
        "    PARQUET\n",
        "  | TEXTFILE\n",
        "  | AVRO\n",
        "  | SEQUENCEFILE\n",
        "  | RCFILE\n",
        "\n",
        "ctas_file_format:\n",
        "    PARQUET\n",
        "  | TEXTFILE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_nUtSSrQv2q",
        "colab_type": "text"
      },
      "source": [
        "### vi\n",
        "\n",
        "![alt text](https://image.slidesharecdn.com/vicheatsheet-190416230629/95/vi-cheat-sheet-1-638.jpg?cb=1555456011)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eavt-mj-QYTy",
        "colab_type": "text"
      },
      "source": [
        "## Study points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Pp26CUQhvR",
        "colab_type": "text"
      },
      "source": [
        "### text/csv file read/write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvSk0RY0PxaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a3ce56ac-11e0-4fb4-c736-b032b453eb9b"
      },
      "source": [
        "!wget --output-document=Crimes_-_2019.csv https://data.cityofchicago.org/api/views/w98m-zvie/rows.csv?accessType=DOWNLOAD"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-03 09:45:04--  https://data.cityofchicago.org/api/views/w98m-zvie/rows.csv?accessType=DOWNLOAD\n",
            "Resolving data.cityofchicago.org (data.cityofchicago.org)... 52.206.140.199, 52.206.68.26, 52.206.140.205\n",
            "Connecting to data.cityofchicago.org (data.cityofchicago.org)|52.206.140.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘Crimes_-_2019.csv’\n",
            "\n",
            "Crimes_-_2019.csv       [          <=>       ]  58.98M  3.08MB/s    in 20s     \n",
            "\n",
            "2020-07-03 09:45:25 (2.99 MB/s) - ‘Crimes_-_2019.csv’ saved [61844522]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGlE5SfjfI5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "a36fef47-5b0f-4298-e261-0c2fe28e5129"
      },
      "source": [
        "df=spark.read.csv(\"Crimes_-_2019.csv\",header=True, inferSchema=True,sep=\",\")\n",
        "df.show()"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------+--------------------+--------------------+----+------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "|      ID|Case Number|                Date|               Block|IUCR|      Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
            "+--------+-----------+--------------------+--------------------+----+------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "|11761076|   JC326952|06/29/2019 08:25:...|   042XX W WILCOX ST|2014|         NARCOTICS|MANUFACTURE / DEL...|VEHICLE NON-COMME...|  true|   false|1115|      11|  28|            26|      18|     1148174|     1898999|2019|07/01/2020 03:48:...|41.878798552|-87.731398505|(41.878798552, -8...|\n",
            "|12092229|   JD279923|12/01/2019 01:00:...|     062XX W 63RD ST|1563|       SEX OFFENSE|CRIMINAL SEXUAL A...|           RESIDENCE| false|   false| 812|       8|  13|            64|      17|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|12092756|   JD280968|11/14/2019 03:15:...|    0000X E HURON ST|1153|DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|                null| false|   false|1834|      18|  42|             8|      11|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|11759636|   JC285931|05/31/2019 09:13:...|   039XX W THOMAS ST|2017|         NARCOTICS|MANUFACTURE / DEL...|               ALLEY|  true|   false|1112|      11|  37|            23|      18|     1149971|     1907052|2019|07/01/2020 03:48:...|41.900862064|-87.724590441|(41.900862064, -8...|\n",
            "|11788468|   JC375483|08/02/2019 02:41:...| 003XX S COLUMBUS DR|2015|         NARCOTICS|MANUFACTURE / DEL...|       PARK PROPERTY| false|   false| 114|       1|  42|            32|      18|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|11791531|   JC370906|07/30/2019 10:25:...|041XX W WEST END AVE|2014|         NARCOTICS|MANUFACTURE / DEL...|               ALLEY| false|   false|1114|      11|  28|            26|      18|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|11759788|   JC322986|06/26/2019 02:25:...|    029XX S STATE ST|2024|         NARCOTICS|POSSESS - HEROIN ...|VEHICLE NON-COMME...|  true|   false| 133|       1|   3|            35|      18|     1176760|     1885391|2019|07/01/2020 03:48:...|41.840859176|-87.626847966|(41.840859176, -8...|\n",
            "|12090929|   JD278760|09/30/2019 03:00:...|     005XX E 38TH ST|1153|DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|                null| false|   false| 212|       2|   4|            35|      11|     1180402|     1879899|2019|07/01/2020 03:48:...|41.825705753|-87.613652178|(41.825705753, -8...|\n",
            "|11791600|   JC368168|07/28/2019 11:30:...|  004XX S PULASKI RD|2017|         NARCOTICS|MANUFACTURE / DEL...|         GAS STATION| false|   false|1132|      11|  28|            26|      18|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|11759846|   JC301932|06/11/2019 02:05:...|    100XX W OHARE ST|1822|         NARCOTICS|MANUFACTURE / DEL...|    AIRPORT/AIRCRAFT|  true|   false|1651|      16|  41|            76|      18|     1100658|     1934241|2019|07/01/2020 03:48:...|41.976290414|-87.905227221|(41.976290414, -8...|\n",
            "|11761071|   JC325517|06/28/2019 10:45:...|042XX W WASHINGTO...|2017|         NARCOTICS|MANUFACTURE / DEL...|RESIDENCE - PORCH...|  true|   false|1114|      11|  28|            26|      18|     1148212|     1900158|2019|07/01/2020 03:48:...|41.881978249|-87.731229119|(41.881978249, -8...|\n",
            "|11761048|   JC345576|07/12/2019 12:24:...| 033XX W CHICAGO AVE|2014|         NARCOTICS|MANUFACTURE / DEL...|VEHICLE NON-COMME...|  true|   false|1121|      11|  27|            23|      18|     1153805|     1905131|2019|07/01/2020 03:48:...| 41.89551514|-87.710559023|(41.89551514, -87...|\n",
            "|12089907|   JD278275|07/09/2019 05:00:...|  035XX N HALSTED ST|1154|DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|  GROCERY FOOD STORE| false|   false|1924|      19|  44|             6|      11|     1170300|     1923541|2019|07/01/2020 03:48:...|41.945688582|-87.649437757|(41.945688582, -8...|\n",
            "|11788438|   JC369446|07/29/2019 10:59:...| 002XX S KILDARE AVE|2014|         NARCOTICS|MANUFACTURE / DEL...|            SIDEWALK| false|   false|1115|      11|  28|            26|      18|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|12088705|   JD276690|11/26/2019 10:00:...|    003XX W 104TH ST|1130|DECEPTIVE PRACTICE|FRAUD OR CONFIDEN...|           RESIDENCE| false|   false| 512|       5|  34|            49|      11|     1176006|     1835964|2019|07/01/2020 03:48:...|41.705243029|-87.631095853|(41.705243029, -8...|\n",
            "|11758550|   JC315872|06/21/2019 03:20:...|  062XX S VERNON AVE|2017|         NARCOTICS|MANUFACTURE / DEL...|VEHICLE NON-COMME...|  true|   false| 313|       3|  20|            42|      18|     1180324|     1863707|2019|07/01/2020 03:48:...|41.781275305|-87.614435197|(41.781275305, -8...|\n",
            "|11759807|   JC324471|05/22/2019 01:40:...|  057XX S CICERO AVE|2091|         NARCOTICS|    FORFEIT PROPERTY|    AIRPORT/AIRCRAFT|  true|   false| 813|       8|  23|            56|      18|     1145654|     1866253|2019|07/01/2020 03:48:...|41.788987036| -87.74147999|(41.788987036, -8...|\n",
            "|11791517|   JC367890|07/28/2019 07:20:...|  002XX N KARLOV AVE|2014|         NARCOTICS|MANUFACTURE / DEL...|            SIDEWALK| false|   false|1114|      11|  28|            26|      18|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "|11760916|   JC345563|07/12/2019 11:53:...|006XX N ST LOUIS AVE|2014|         NARCOTICS|MANUFACTURE / DEL...|            SIDEWALK|  true|   false|1121|      11|  27|            23|      18|     1152933|     1904102|2019|07/01/2020 03:48:...|41.892708787|-87.713788982|(41.892708787, -8...|\n",
            "|11791619|   JC369182|07/29/2019 08:53:...|  001XX N KEELER AVE|2014|         NARCOTICS|MANUFACTURE / DEL...|            SIDEWALK| false|   false|1114|      11|  28|            26|      18|        null|        null|2019|07/01/2020 03:51:...|        null|         null|                null|\n",
            "+--------+-----------+--------------------+--------------------+----+------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzH2hhTIfqlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.createOrReplaceTempView(\"crime\")"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRXRRB3ZgElV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "bd262c9e-58d8-4c98-f34b-a94afdecbf0d"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Case Number: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Block: string (nullable = true)\n",
            " |-- IUCR: string (nullable = true)\n",
            " |-- Primary Type: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Location Description: string (nullable = true)\n",
            " |-- Arrest: boolean (nullable = true)\n",
            " |-- Domestic: boolean (nullable = true)\n",
            " |-- Beat: integer (nullable = true)\n",
            " |-- District: integer (nullable = true)\n",
            " |-- Ward: integer (nullable = true)\n",
            " |-- Community Area: integer (nullable = true)\n",
            " |-- FBI Code: string (nullable = true)\n",
            " |-- X Coordinate: integer (nullable = true)\n",
            " |-- Y Coordinate: integer (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Updated On: string (nullable = true)\n",
            " |-- Latitude: double (nullable = true)\n",
            " |-- Longitude: double (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StECV6v9uDmV",
        "colab_type": "text"
      },
      "source": [
        "### Concat, concat_ws work!\n",
        "\n",
        "Don't use \"+\"!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_3YguWerF5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "e7168630-18d8-463f-e22b-fd10bf8db26f"
      },
      "source": [
        "spark.sql(\"select concat_ws('###',`Case Number`, Date) as t from crime\").show()"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                   t|\n",
            "+--------------------+\n",
            "|JC326952###06/29/...|\n",
            "|JD279923###12/01/...|\n",
            "|JD280968###11/14/...|\n",
            "|JC285931###05/31/...|\n",
            "|JC375483###08/02/...|\n",
            "|JC370906###07/30/...|\n",
            "|JC322986###06/26/...|\n",
            "|JD278760###09/30/...|\n",
            "|JC368168###07/28/...|\n",
            "|JC301932###06/11/...|\n",
            "|JC325517###06/28/...|\n",
            "|JC345576###07/12/...|\n",
            "|JD278275###07/09/...|\n",
            "|JC369446###07/29/...|\n",
            "|JD276690###11/26/...|\n",
            "|JC315872###06/21/...|\n",
            "|JC324471###05/22/...|\n",
            "|JC367890###07/28/...|\n",
            "|JC345563###07/12/...|\n",
            "|JC369182###07/29/...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i2MerFoKQaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result1=spark.sql(\"select date_format(to_date(date,'MM/dd/yyyy'),'yyyyMM') as month, `primary type`, count(1) as counts from crime group by month, `primary type` order by month, counts desc\")\n",
        "\n",
        "result1.rdd.map(lambda rec: '\\t'.join([str(r) for r in rec])).coalesce(1).saveAsTextFile(\"text1.gzip\",compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNKmisxwNW5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result1=spark.sql(\"select date_format(to_date(date,'MM/dd/yyyy'),'yyyyMM') as month, `primary type`, count(*) as counts from crime group by month, `primary type` order by month, counts desc\")\n",
        "\n",
        "result1_rdd=result1.rdd.map(lambda rec: '\\t'.join([str(r) for r in rec]))\n",
        "\n",
        "result1_rdd.coalesce(1).saveAsTextFile(\"text6.gzip\",compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMLQwOhTHL3L",
        "colab_type": "text"
      },
      "source": [
        "### sql write text does work TOO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL9XbowuBJdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result1.createOrReplaceTempView(\"result1\")\n",
        "result2=spark.sql(\"select concat_ws('\\t',month, `primary type`, CAST(counts as string)) as value from result1\")"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs7dBDy7FZ2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "36f072f3-7b56-4489-8955-a07cad8c6453"
      },
      "source": [
        "result2.show()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|   201901\tTHEFT\t4554|\n",
            "| 201901\tBATTERY\t3519|\n",
            "|201901\tCRIMINAL D...|\n",
            "|201901\tDECEPTIVE ...|\n",
            "| 201901\tASSAULT\t1458|\n",
            "|201901\tOTHER OFFE...|\n",
            "|201901\tNARCOTICS\t...|\n",
            "| 201901\tBURGLARY\t797|\n",
            "|201901\tMOTOR VEHI...|\n",
            "|  201901\tROBBERY\t658|\n",
            "|201901\tCRIMINAL T...|\n",
            "|201901\tWEAPONS VI...|\n",
            "|201901\tOFFENSE IN...|\n",
            "|201901\tCRIM SEXUA...|\n",
            "|201901\tINTERFEREN...|\n",
            "|201901\tSEX OFFENS...|\n",
            "|201901\tPUBLIC PEA...|\n",
            "|201901\tPROSTITUTI...|\n",
            "|201901\tCRIMINAL S...|\n",
            "|  201901\tHOMICIDE\t23|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo22NkVS_S3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2.coalesce(1).write.options(compression=\"gzip\").text(\"sql_test3.gzip\")\n"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xquq_SBFStt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2.coalesce(1).write.text(\"text1.txt\")"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jm5ESTvfjnc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b9c99403-27fa-44f9-aa55-ef18a7a1d138"
      },
      "source": [
        "spark.sql(\"select `primary type`, count(*) as counts from crime group by `primary type` having counts<100 order by counts limit 2\").show()\n",
        "\n",
        "# limit 2 at the ending to select top 2\n",
        "# top 2 at the beginning is NOT valid"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|        primary type|counts|\n",
            "+--------------------+------+\n",
            "|        NON-CRIMINAL|     4|\n",
            "|OTHER NARCOTIC VI...|     8|\n",
            "+--------------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZDym84M-u8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql5=spark.sql(\"select `primary type`, count(*) as counts from crime group by `primary type` having counts<100 order by counts limit 2\")\n",
        "#sql5.write.csv(\"sql6.csv\", sep=\"\\t\", header=True) # Sep header"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvOe_5yUC6rV",
        "colab_type": "text"
      },
      "source": [
        "# Now we can practice Spark -- RDD\n",
        "<Font color=red>**Please don't spend too much time on RDD, which is not required for the exam CCA Spark and Hadoop Developer Exam (CCA175).**</Font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6IHkcETHMMA",
        "colab_type": "text"
      },
      "source": [
        "## Data - Order-OrderItems-Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c6UvPyb2HGTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "bbc32e18-bef4-4859-d3bb-d91bb2be21f4"
      },
      "source": [
        "print_top10(products)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Product(product_id=1, product_category_id=2, product_name='Quest Q64 10 FT. x 10 FT. Slant Leg Instant U', product_description='', product_price=59.98)\n",
            "Product(product_id=2, product_category_id=2, product_name=\"Under Armour Men's Highlight MC Football Clea\", product_description='', product_price=129.99)\n",
            "Product(product_id=3, product_category_id=2, product_name=\"Under Armour Men's Renegade D Mid Football Cl\", product_description='', product_price=89.99)\n",
            "Product(product_id=4, product_category_id=2, product_name=\"Under Armour Men's Renegade D Mid Football Cl\", product_description='', product_price=89.99)\n",
            "Product(product_id=5, product_category_id=2, product_name='Riddell Youth Revolution Speed Custom Footbal', product_description='', product_price=199.99)\n",
            "Product(product_id=6, product_category_id=2, product_name=\"Jordan Men's VI Retro TD Football Cleat\", product_description='', product_price=134.99)\n",
            "Product(product_id=7, product_category_id=2, product_name='Schutt Youth Recruit Hybrid Custom Football H', product_description='', product_price=99.99)\n",
            "Product(product_id=8, product_category_id=2, product_name=\"Nike Men's Vapor Carbon Elite TD Football Cle\", product_description='', product_price=129.99)\n",
            "Product(product_id=9, product_category_id=2, product_name='Nike Adult Vapor Jet 3.0 Receiver Gloves', product_description='', product_price=50.0)\n",
            "Product(product_id=10, product_category_id=2, product_name=\"Under Armour Men's Highlight MC Football Clea\", product_description='', product_price=129.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_onqFscfHBzK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "bca55387-a519-4d70-d0ae-959ebf195bbd"
      },
      "source": [
        "print_top10(orders)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order(order_id=1, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11599, order_status='CLOSED')\n",
            "Order(order_id=2, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=256, order_status='PENDING_PAYMENT')\n",
            "Order(order_id=3, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=12111, order_status='COMPLETE')\n",
            "Order(order_id=4, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=8827, order_status='CLOSED')\n",
            "Order(order_id=5, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11318, order_status='COMPLETE')\n",
            "Order(order_id=6, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=7130, order_status='COMPLETE')\n",
            "Order(order_id=7, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=4530, order_status='COMPLETE')\n",
            "Order(order_id=8, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=2911, order_status='PROCESSING')\n",
            "Order(order_id=9, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=5657, order_status='PENDING_PAYMENT')\n",
            "Order(order_id=10, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=5648, order_status='PENDING_PAYMENT')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XMNT6u4fG-BI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "a429ee41-f78c-41ad-b98f-eefba9619189"
      },
      "source": [
        "print_top10(order_Items)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order_Items(order_item_id=1, order_item_order_id=1, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=2, order_item_order_id=2, order_item_product_id=1073, order_item_quantity=1, order_item_subtotal=199.99, order_item_product_price=199.99)\n",
            "Order_Items(order_item_id=3, order_item_order_id=2, order_item_product_id=502, order_item_quantity=5, order_item_subtotal=250.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=4, order_item_order_id=2, order_item_product_id=403, order_item_quantity=1, order_item_subtotal=129.99, order_item_product_price=129.99)\n",
            "Order_Items(order_item_id=5, order_item_order_id=4, order_item_product_id=897, order_item_quantity=2, order_item_subtotal=49.98, order_item_product_price=24.99)\n",
            "Order_Items(order_item_id=6, order_item_order_id=4, order_item_product_id=365, order_item_quantity=5, order_item_subtotal=299.95, order_item_product_price=59.99)\n",
            "Order_Items(order_item_id=7, order_item_order_id=4, order_item_product_id=502, order_item_quantity=3, order_item_subtotal=150.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=8, order_item_order_id=4, order_item_product_id=1014, order_item_quantity=4, order_item_subtotal=199.92, order_item_product_price=49.98)\n",
            "Order_Items(order_item_id=9, order_item_order_id=5, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=10, order_item_order_id=5, order_item_product_id=365, order_item_quantity=5, order_item_subtotal=299.95, order_item_product_price=59.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRtXKQdbHhd4",
        "colab_type": "text"
      },
      "source": [
        "## Spark RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxr1-F9aHq30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n",
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ZYNKrFYi3F",
        "colab_type": "text"
      },
      "source": [
        " However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: **rdd.collect().foreach(println).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORGXOYC_Y_En",
        "colab_type": "text"
      },
      "source": [
        "## Transformations\n",
        "The following table lists some of the common transformations supported by Spark. Refer to the RDD API doc (Scala, Java, Python, R) and pair RDD functions doc (Scala, Java) for details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW_pHyeVZQJm",
        "colab_type": "text"
      },
      "source": [
        "### map(func)\t\n",
        "Return a new distributed dataset formed by passing each element of the source through a function func."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DlkJF6bYg3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c7a69c49-03ae-4243-f942-b8c934bc11b1"
      },
      "source": [
        "products_short=products.map(lambda p: (p.product_id, p.product_name))\n",
        "\n",
        "print_top10(products_short)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 'Quest Q64 10 FT. x 10 FT. Slant Leg Instant U')\n",
            "(2, \"Under Armour Men's Highlight MC Football Clea\")\n",
            "(3, \"Under Armour Men's Renegade D Mid Football Cl\")\n",
            "(4, \"Under Armour Men's Renegade D Mid Football Cl\")\n",
            "(5, 'Riddell Youth Revolution Speed Custom Footbal')\n",
            "(6, \"Jordan Men's VI Retro TD Football Cleat\")\n",
            "(7, 'Schutt Youth Recruit Hybrid Custom Football H')\n",
            "(8, \"Nike Men's Vapor Carbon Elite TD Football Cle\")\n",
            "(9, 'Nike Adult Vapor Jet 3.0 Receiver Gloves')\n",
            "(10, \"Under Armour Men's Highlight MC Football Clea\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdfUzRDPaEA8",
        "colab_type": "text"
      },
      "source": [
        "### filter(func)\n",
        "Return a new dataset formed by selecting those elements of the source on which func returns true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vS3AtGxaLAu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1505497d-60fd-4109-f072-628e1ceac59f"
      },
      "source": [
        "filtered=order_Items.filter(lambda o: o.order_item_product_id==957)\n",
        "\n",
        "print_top10(filtered)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order_Items(order_item_id=1, order_item_order_id=1, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=9, order_item_order_id=5, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=12, order_item_order_id=5, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=15, order_item_order_id=7, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=34, order_item_order_id=12, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=59, order_item_order_id=19, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=68, order_item_order_id=23, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=79, order_item_order_id=28, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=80, order_item_order_id=28, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=94, order_item_order_id=34, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwqN2EPybIoX",
        "colab_type": "text"
      },
      "source": [
        "### flatMap(func)\t\n",
        "Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZ62xI5bSKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ab7b5aba-f33a-4acc-c793-a26bc8233cf8"
      },
      "source": [
        "print_top10( products_txt.flatMap(lambda p: p.split(\",\")))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "Quest Q64 10 FT. x 10 FT. Slant Leg Instant U\n",
            "\n",
            "59.98\n",
            "http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy\n",
            "2\n",
            "2\n",
            "Under Armour Men's Highlight MC Football Clea\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tRDtWDwdDAN",
        "colab_type": "text"
      },
      "source": [
        "### union(otherDataset)\tintersection(otherDataset) distinct([numPartitions]))\n",
        "\n",
        "\n",
        "union(otherDataset) Return a new dataset that contains the union of the elements in the source dataset and the argument.\n",
        "\n",
        "intersection(otherDataset)\tReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
        "\n",
        "distinct([numPartitions]))\tReturn a new dataset that contains the distinct elements of the source dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_8zG7X2dXp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "00fe3eaa-cab9-4203-b13a-5d5d66287ec0"
      },
      "source": [
        "print_top10(order_Items)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order_Items(order_item_id=1, order_item_order_id=1, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=2, order_item_order_id=2, order_item_product_id=1073, order_item_quantity=1, order_item_subtotal=199.99, order_item_product_price=199.99)\n",
            "Order_Items(order_item_id=3, order_item_order_id=2, order_item_product_id=502, order_item_quantity=5, order_item_subtotal=250.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=4, order_item_order_id=2, order_item_product_id=403, order_item_quantity=1, order_item_subtotal=129.99, order_item_product_price=129.99)\n",
            "Order_Items(order_item_id=5, order_item_order_id=4, order_item_product_id=897, order_item_quantity=2, order_item_subtotal=49.98, order_item_product_price=24.99)\n",
            "Order_Items(order_item_id=6, order_item_order_id=4, order_item_product_id=365, order_item_quantity=5, order_item_subtotal=299.95, order_item_product_price=59.99)\n",
            "Order_Items(order_item_id=7, order_item_order_id=4, order_item_product_id=502, order_item_quantity=3, order_item_subtotal=150.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=8, order_item_order_id=4, order_item_product_id=1014, order_item_quantity=4, order_item_subtotal=199.92, order_item_product_price=49.98)\n",
            "Order_Items(order_item_id=9, order_item_order_id=5, order_item_product_id=957, order_item_quantity=1, order_item_subtotal=299.98, order_item_product_price=299.98)\n",
            "Order_Items(order_item_id=10, order_item_order_id=5, order_item_product_id=365, order_item_quantity=5, order_item_subtotal=299.95, order_item_product_price=59.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIxtxbEGdfag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d1e2c54a-7374-4ab0-f4f7-0dd46cffd1bb"
      },
      "source": [
        "print(\"P1=============\")\n",
        "p1=order_Items.filter(lambda i: i.order_item_order_id==1).map(lambda i: i.order_item_product_id)\n",
        "print_top10(p1)\n",
        "print(\"P2=============\")\n",
        "p2=order_Items.filter(lambda i: i.order_item_order_id==5).map(lambda i: i.order_item_product_id)\n",
        "print_top10(p2)\n",
        "print(\"p1.union(p2).distinct()=============\")\n",
        "print_top10(p1.union(p2).distinct()) # distinct is needed to avoid duplicates\n",
        "print(\"p1.intersection(p2).distinct()=============\")\n",
        "print_top10(p1.intersection(p2).distinct())"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P1=============\n",
            "957\n",
            "P2=============\n",
            "957\n",
            "365\n",
            "1014\n",
            "957\n",
            "403\n",
            "p1.union(p2).distinct()=============\n",
            "957\n",
            "365\n",
            "1014\n",
            "403\n",
            "p1.intersection(p2).distinct()=============\n",
            "957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn48sRpngAcf",
        "colab_type": "text"
      },
      "source": [
        "### groupByKey([numPartitions])\t\n",
        "\n",
        "When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
        "Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
        "Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjae92vAgXn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fe6f8f2e-b01c-4521-a06b-9dfc3d072ee2"
      },
      "source": [
        "Order_Items.order_item_order_id"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<property at 0x7f94acec1368>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Mb0ZQzgFju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "660f5740-bf21-4459-b410-3f56daef4fa6"
      },
      "source": [
        "print_top10(order_Items.map(lambda o: (o.order_item_order_id, \\\n",
        "            o.order_item_product_id)).groupByKey())"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8d390>)\n",
            "(4, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8d2b0>)\n",
            "(8, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8d5c0>)\n",
            "(10, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8d438>)\n",
            "(12, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8d3c8>)\n",
            "(14, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8dcf8>)\n",
            "(16, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8d160>)\n",
            "(18, <pyspark.resultiterable.ResultIterable object at 0x7f94ace8df28>)\n",
            "(20, <pyspark.resultiterable.ResultIterable object at 0x7f94adf161d0>)\n",
            "(24, <pyspark.resultiterable.ResultIterable object at 0x7f94acf99080>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvucM-3DhJA1",
        "colab_type": "text"
      },
      "source": [
        "### reduceByKey(func, [numPartitions])\n",
        "\n",
        "When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFPD6nFLlcgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d50e0af5-15e4-4910-fa80-7410dc98d8ad"
      },
      "source": [
        "print_top10(order_Items.filter(lambda o: (o.order_item_order_id==2)))"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order_Items(order_item_id=2, order_item_order_id=2, order_item_product_id=1073, order_item_quantity=1, order_item_subtotal=199.99, order_item_product_price=199.99)\n",
            "Order_Items(order_item_id=3, order_item_order_id=2, order_item_product_id=502, order_item_quantity=5, order_item_subtotal=250.0, order_item_product_price=50.0)\n",
            "Order_Items(order_item_id=4, order_item_order_id=2, order_item_product_id=403, order_item_quantity=1, order_item_subtotal=129.99, order_item_product_price=129.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ak9Ax_hSJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "367b3ede-553f-4d9d-f7de-6dab5aaefc60"
      },
      "source": [
        "print_top10(order_Items.map(lambda o: (o.order_item_order_id, \\\n",
        "            o.order_item_product_id)).reduceByKey(lambda x, y: max(x,y)))"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1073)\n",
            "(4, 1014)\n",
            "(8, 1014)\n",
            "(10, 1073)\n",
            "(12, 1014)\n",
            "(14, 1014)\n",
            "(16, 365)\n",
            "(18, 1073)\n",
            "(20, 1014)\n",
            "(24, 1073)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtZ17QHi1Da",
        "colab_type": "text"
      },
      "source": [
        "### aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])\t\n",
        "\n",
        "When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNhD7FjZh5rw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8fa6cf97-46fb-44b3-a9fa-62ca24484c28"
      },
      "source": [
        "print_top10(order_Items.map(lambda o: (o.order_item_order_id, \\\n",
        "            o.order_item_product_id)).aggregateByKey(0,(lambda x,y: x+1), (lambda x,y: x+1)))"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n",
            "(4, 4)\n",
            "(8, 4)\n",
            "(10, 5)\n",
            "(12, 5)\n",
            "(14, 3)\n",
            "(16, 2)\n",
            "(18, 3)\n",
            "(20, 4)\n",
            "(24, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqQhbwywnbTs",
        "colab_type": "text"
      },
      "source": [
        "### sortByKey([ascending], [numPartitions])\t\n",
        "When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkiYXLwwlRig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c24df2e5-0e20-4387-a4b9-6de3940cf59c"
      },
      "source": [
        "print_top10(order_Items.map(lambda o: (o.order_item_order_id, \\\n",
        "            o.order_item_product_id)).sortByKey(False))"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(68883, 208)\n",
            "(68883, 502)\n",
            "(68882, 365)\n",
            "(68882, 502)\n",
            "(68881, 403)\n",
            "(68880, 1014)\n",
            "(68880, 502)\n",
            "(68880, 1073)\n",
            "(68880, 1014)\n",
            "(68880, 1014)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nl1ylgZrz-E",
        "colab_type": "text"
      },
      "source": [
        "### join(otherDataset, [numPartitions])\t\n",
        "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOZHstBwr8nX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "dfccf65e-202c-4a0d-aa05-1116f6a65038"
      },
      "source": [
        "o_by_k=order_Items.map(lambda o: (o.order_item_product_id, o.order_item_order_id)).filter(lambda x: x[0]==957)\n",
        "print_top10(o_by_k)\n"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(957, 1)\n",
            "(957, 5)\n",
            "(957, 5)\n",
            "(957, 7)\n",
            "(957, 12)\n",
            "(957, 19)\n",
            "(957, 23)\n",
            "(957, 28)\n",
            "(957, 28)\n",
            "(957, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4weGf_NLXMbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "070cfe31-aaf7-4230-d846-93b83582c9df"
      },
      "source": [
        "p_by_k=products.filter(lambda p: p.product_id==957).map(lambda p: (p.product_id, p.product_name))\n",
        "print_top10(p_by_k)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(957, \"Diamondback Women's Serene Classic Comfort Bi\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQhmTXDVW_eM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "af5ae80a-ecee-4e38-c244-f147881b1433"
      },
      "source": [
        "j_by_k=o_by_k.join(p_by_k)\n",
        "# print(j_by_k.first())\n",
        "\n",
        "print_top10(j_by_k.\\\n",
        "  map(lambda l: (l[0],l[1][0], l[1][1])))"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(957, 1, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 5, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 5, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 7, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 12, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 19, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 23, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 28, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 28, \"Diamondback Women's Serene Classic Comfort Bi\")\n",
            "(957, 34, \"Diamondback Women's Serene Classic Comfort Bi\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoUQ14EaHr3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 226,
      "outputs": []
    }
  ]
}